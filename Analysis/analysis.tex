%!TeX root=../Dissertation.tex
%!TeX bibfile=./analysis.bib




\chapter{Review of Virtualisation}
\label{chap:virtualisationReview}

\section{Terminology \& Definitions}


\subsection{Virtualisation}
Virtualisation as a term originated in the 1960s when IBM workers began work on a project that would allow an IBM model-40 computer to segregate off its memory and allow up to 15 users to use the computer independently at once \citep{Lindquist1966}. Each user would see their own abstract Operating System, separate from the others. Whilst virtualisation has continued in its development from this point onwards, the core functionality and architecture behind Hardware Virtualisation, remain much the same.

Each of these individual logical (as opposed to physical) devices is called a `virtual machine'.

\subsection{Hardware Virtualisation}
\label{subsec:HardwareVirtualisation}
There are a number of different types of virtualisation. This can open the scope for what can and can not be considered as a virtual machine. For the purpose of this report the term `virtualisation' will refer specifically to `hardware based virtualisation', whereby a `virtual machine' is an operating system running on top of another operating system, with the virtualisation task itself being controlled by a `hypervisor' (This is explained in more detail in the next subsection: \ref{subsec:hypervisor}).

This is an important definition to clarify, as often times containerisation (subsection \ref{sec:containerisation}) can be viewed as a type of virtualisation, and other times not. For the purposes of this report, the two definitions must be distinguished as separate things, much like in the report ``Autonomic Orchestration of Containers: Problem Definition and Research Challenges'' \citep{casalicchio2016}, where a clear and defined difference between Hardware Virtualisation and Containerisation is made.

\subsection{Hypervisor}
\label{subsec:hypervisor}
Hardware Virtualisation relies on an underlying software that runs on the already existing OS in order to manage each instance/OS. This software can have a number of different names depending on the origin of the work, and the context. In early work on virtualisation, this software was often referred to simply as a ``control program'' \citep{creasy1981}, but for the purposes of this research, this software will be referred to as a `Hypervisor'. This term is often preferred in practical settings, such as in VMware's online Glossary \citep{vmwareHypervisor}, or in Red Hat's ``What is a hypervisor'' \citep{redhat2021}.



\section{The workings of virtualisation}
Virtualisation works






\chapter{Review of Containerisation}
\label{chap:containerisationReview}

\section{Terminology \& Definitions}

\subsection{Containerisation}
\label{subsec:containerisation}
Containerisation has become the de-facto term to describe what could also be described as OS-level virtualisation. For the purposes of this report, I will be referring to this technology only as Containerisation, and each individual instance as a container (instead of virtual machine). This is the same approach towards defining containers as taken by Dua et al \citep{dua14} when they have made their own distinction between Containers and Virtual Machines.

\subsection{Container Engine}
\label{Container Engine}
What a hypervisor is to a virtual machine, a container engine is to a container. A container engine sits on the base operating system, much the same as a hypervisor. Where a difference is found however, is in the way it interacts with the base operating system.

Whilst a hypervisor works by running a full operating system on top of the existing one, a container engine works without that upper operating system by using the base operating system as the OS component for each container. Segregation of containers and resource allocation is simply managed by the container engine, so that each container is allocated resources. This can be done dynamically or be static, depending on the use-case.




\section{Developments in containerisation technology}

\subsection{Docker}
\label{subsec:docker}
An ever more popular implementation of container technology is a software known as Docker. This software is primarily used by (and aimed at) developers, who can use the platform to quickly create and deploy applications for testing. The use of containers is supposed to make managing these applications much easier, and ensures compatibility along the whole development process.
Datanyze (a technology market usage group) report that Docker is currently the second most used Containerisation platform, with 25.34\% market share in Containerisation \citep{datanyze}. 

Docker's Container engine is aptly named Docker Engine, and utilises a client-server architecture \citep[Section: Docker architecture]{DockerOverview}. The server portion of the Docker System is known as the `docker daemon', the Client uses a command line interface to interact with one or more docker daemons. The client and daemon communicate using a `REST API' \citep[Section: The Docker daemon]{DockerOverview}. REST (Representational State Transfer) provides an architecture for web services \citep{W3Architecture2004} that allows them to communicate over HTTP. The protocols within REST are stateless, this means there is no set `state' or session control within the protocol; each command sent to a daemon can be understood as it is, without the need for any outside context to the command being sent.

The Docker Daemon manages `Objects' \citep[Section: Docker objects]{DockerOverview} required for a full Docker system. 'Objects' refers to the containers, the images (Docker images are the instruction sets for Docker containers, not to be confused with OS images, though often the Docker images will include which OS image is to be used).

Docker images are stored in a Docker Registry \citep[Section: Docker registries]{DockerOverview}. By default, the registry is configured to use "Docker Hub" which is a public, open registry that already contains a large number of complete images for use. This registry can be changed however, to point to any location. This behaviour allows a registry to be setup part of a network, and the images can then be 'pulled' or 'run' from the registry using the "docker pull" and "docker run" commands\citep[Section: Docker registries]{DockerOverview}. An image can also be configured on a live container, and then that image pushed to the registry using the "docker push" command \citep[Section: Docker registries]{DockerOverview}.

\subsection{How Docker works}
Docker is written using the 'Go Programming language' \citep[Section: The underlying technology]{DockerOverview}. Go is developed and maintained by Google on an open source liscense, and is roughly based on the C programming language\citep{GoAncestors}. Being based on C gives the programming language the same benefits of any other low level language, being able to make use of functions that are integral to the kernal and operating system. Where go differs is that it is also designed to be be more intuitive and "clutter free" \citep{GoPrinciples}.

\subsubsection{Namespaces}
Docker makes use of a feature of Go that allows further use of a linux kernal feature known as namespaces \citep[Section: Namespaces]{DockerOverview}.
When new containers are created, a set of namespaces are created sepcifically for that container. This means that programs that might otherwise be considered by the kernal to be entirely sperate, are processed together, and vice versa. This in turn allows multiple containers to run processes in isolation that otherwise would have been processed by the kernal together, and also process a number of actions as one whole unit, that otherwise would have been considered seperate tasks to the operating system. This is key to the function of containers, as it allows each container to process tasks as sperate entities, but make use of the same kernal and operating system across all containers.

\subsubsection{Control Groups}
Control groups is another feature of the linux kernal used by Docker. Control groups allow hardware resources to be segrated in a way that limits and further isolates them \citep{corbetControlGroups}. In docker, this is used to segregate parts of memory, logical processors, drive space and network access so that there is no crossover in hardware utilisation between different containers. This is similar to how a hypervisor would segregate resources, but instead this is managed entirely by the kernal.

\chapter{System design and definition}


\section{Maintaining scientific method}
To ensure that results are scientific, variables must be controlled between both of the systems. The first step in this, is ensuring that the topology and configurations for both systems are the same. This can be done by copying the configuration files from one system to the other, ensuring that the system works in the same way for both systems. As the underlying operating system should be a version of Linux for both the virtual machine and the Docker system, this should be relatively easy.

To further ensure that variables are controlled, we need to ensure that the same benchmarks are maintained throughout the testing process, when being used on the \emph{same part of the system}. This means that if one benchmark is used to measure, for example, network latency on a web-server, then the same benchmark should be used to measure the network latency on that same web-server on the mirrored system. It may be necessary to use different benchmarks across the whole system, but this is acceptable as long as all testing is done to a parallel across both systems. The benchmarks to be used will be discussed in more detail in section \ref{sec:Benchmarking} (Benchmarking).


\section{LAMP System}
I will need to make sure that the test system is as accurate to a real-world system as possible. To do this I will be employing a LAMP topology for both the Docker system and the Virtual Machine system. LAMP is the acronym for Linux, Apache, MySQL and PHP. Whilst this can technically be run all on one system, it is common to separate various functions out onto different machines (logically or physically), this generally makes management of these systems easier.

For the linux section of the LAMP topology, I will be employing Ubunutu Server as it is headless which resulting in a lower overhead, and because I personally familiar with Ubuntu as an operating system. Using Ubuntu is also good as an example, as it is a widely available and utilised operating system among those that run linux servers. Ubuntu even has its own images stored officially on Docker hub, which is updated regularly to stay in line with Ubuntu's Long Term Service version \citep{ubuntuDocker}. Some of the images hosted on Docker Hub by Ubuntu \citep{ubuntuDockerProfile} are already configured to contain some of the parts required for the depolyment, such as Apache2 and MySQL. These may be useful when I come to implement my own Docker setup, though for the sake of ensuring a fair-test, I may instead push images to docker that use the exact setup used by my virtual-machine testing. This could remove a possible source of extraneous variables from the testing.

\section{Benchmarking}
\label{sec:Benchmarking}
Benchmarking software and tools are designed to create a standard output measurement for performance of a computer-based system\citep{fleming1986}.

There are a  number of benchmarking tools available, but for this research these can be split in discussion along lines of what they are designed to measure. That being said, the main split for this research is the measuring of computing performance, and of network performance.

\subsection{Computing Performance}
Computing performance in this case relates to the performance as a result of the computers ability to process information, and at what rate. Whilst this is tied directly to the processor, RAM, and other hardware, the overhead of the Operating system can affect these components and as a result, have a large affect on the performance of a system. This effect is known as `operating system overhead'. Based on previous research on containers and virtual machines it can be hypothesised that in this research, the total Operating System overhead for a container-based system will be smaller than that of Virtual Machines \emph{(when using the same operating system)}. This is due to the way that containers utilise one OS for their function (as discussed in subsection \ref{Container Engine}).

\subsection{Network Performance}
Further to the Computing Performance; Networking Performance is the measured performance on the network between various nodes. These nodes are the servers, clients and other infrastructure that are configured to receive and send traffic on a network. One of the best ways to measure network performance is delay. Compute performance may change the network delay in some areas, but not all. To explore this, we should further break network delay into its four main components:
\begin{itemize}
  \item Processing Delay: The amount of time it takes for a node (router, server, client, etc) to process the header of a packetz \citep{ProcessingDelay}. This is usually affected by the CPU performance, which is linked to the compute performance as discussed previously.
  \item Queing Delay: The time spent after being processed or produced by the node, and then actually being pushed onto the line \citep{QueuingDelay}. This is called queing delay, as this delay is most usually due to other data already being pushed out onto the line. As a result, our data is waiting in a 'queue' behind other data, waiting to be pushed onto the line. This is affected by the amount of traffic being sent from a node, along with the speed at which the node can process mutliple packets. This does tie in with Computing performance.
  \item Transmission Delay: The amount of time it takes for a node to 'push' packets (bit by bit) onto the line \citep[Chapter 7]{chen2005}. This delay is a result of the bandwidth on a line, and as a result, the change in transmission delay between virtualisation and containerisation can't be hypothesised. This is due to differences in the way that both methods manage network traffic. There is no physical line between the servers, so differences in bandwidth are entirely down to the relevant solutution (Containerisation, or virtualisation).
  \item Propogation Delay: The time it takes for packets to travel over a line/medium (such as copper cable) \citep{PropogationDelay}. This could be affected by the the change from virtualisation to containerisation, however, this would be down to the individual ways that each of the soltutions I choose manage network traffic between logical machines. As such, I can't generate a hypothesis for how this would be affected, much the same as Transmission Delay.

\subsection{Other key findings}
Whilst the purpose of this project will be primarily to find differences in speed between the two methods, there may be other performance, or even logistical improvements of one over the other. The final output from this research should be a reccomendation to those that might be considering using containers in order to run their infrastructure, so whilst performance might be the key focus of this report, I will ensure to mention in evaluation anything else that could be considered important to those that might be looking to containerisation as a 'step forward' for infrasturcuture. For example, in subsection \ref{subsec:docker} (Docker) I discuss the Docker Hub, which is an public repository of images that anyone can push to or pull from. This system might make the deployment of containers easier than the deployment of virtual machines, so whilst this isn't directly related to the performance of the system itself, it may be worth mentioning as a pssible point of interest.
\end{itemize}
%UNFINISHED Find a reference for each of the above^

We can see from exploring these various types of delay on the network, that network delay can be directly tied to Computing Performance in \emph{some} areas. As a result of this, it could be expected that the changes to overhead, and the possibility of streamlining processes as discussed in chapters \ref{chap:virtualisationReview} and \ref{chap:containerisationReview}.


\subsection{DNS}
%USe BIND9 for opensource DNS config.
\subsection{Webserver}

\subsection{Intranet}

\chapter{How will the system be measured}
%Talk about benchmarks.