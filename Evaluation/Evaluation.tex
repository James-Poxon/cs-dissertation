%!TeX root=../Dissertation.tex
%!TeX bibfile=./synthesis.bib

%------

\chapter{Evaluation of the Product}
%Do a little preamble about success and important results to get us started
\section{Evaluation of the systems}
\subsection{Successes}
Both systems (The VMware system and the Docker system) do exactly what was described in the analysis. The network topology for both of the real systems is exactly the same, with both of these networks matching the topology that was designed during the analysis section (Figure \ref{fig:Topology}, in subsection \ref{subsec:TopologyDiagram}). The system maintained a LAMP topology, as was specified, along with the various other components of the system such as DHCP and DNS. This is an important part of the research because it demonstrates that this kind of system can be supported by containers, and not just virtual machines, as is done historically. Furthermore, the configuration for matching services across the two systems (ie. DHCP on VMware; DHCP on Docker) are identical, ensuring that results remained comparable.

Separately, the VMware system is not particularly impressive, or groundbreaking, whilst the container based system on the other hand \emph{is} impressive, and potentially groundbreaking. This is because there are not many examples of systems such as this one being built for use outside of small scale development operations (whereby the network infrastructure couldn't be supported on a real LAN). Together, the two systems combine to form an important base for research that could potentially impact the current paradigm of virtualisation in which we currently find ourselves in. The actual results, and their impact will be discussed further in section \ref{sec:EvaluationTestResults}, but without the work that was conducted to ensure that both systems worked exactly as designed, those results wouldn't have been useful. This is due to the fact that the testing was a direct comparison of systems; should there have been any disparity between the VMware network and the Docker network, the results would not be a fair comparison.

This required a level of perseverance when it came to finding something within the Docker system that was difficult. For example, getting phpMyAdmin to work on the Docker system wasn't as easy as the rest of the build (as mentioned in subsection \ref{subsec:softwaresynth}), but a workaround was found to make it work, even though there was probably an alternative system that could have been implemented easier. Had the Docker system been the only system being built, and that had been the project basis, then using an alternative method would have been acceptable, and this could have been worked into the synthesis, but with both systems needing to be identical, this was just not an option. In the long run, I think this has made the project more of a success, because it has proven that taking systems that already exist within virtual environments, and converting them to container environments is an option without compromise for those that are in positions to make this move.

The scalability of the system we have created is very good. During development the system was actually entirely designed and created on a separate machine and then moved over to the testing machine after the fact with a fresh installation of Ubuntu. Setting the whole system up on a fresh machine was simple, and there was minimal hitches in the process. This suggests that both the containers and virtual machines were designed and created in an efficient manner. I would suggest that scaling up the number of machines, or adding new functions to the system later, would be an easy task.

\subsection{Some notable Limitations}%TODO talk about docker on linux not being able to run windows server. Can docker on windows support windows server? You did that so that you didn't have problems with WSL2.
\label{HardwareLimitations}
This project took place over what became a bizarre year for academia. Due to the Coronavirus pandemic, there was limited ability to access network labs and University infrastructure, so this project had to be scaled in a way that was reasonable for me to complete with my own personal hardware.

\subsubsection{RAM and CPU usage}
\label{RAMCPU}
The computer that was used was a desktop PC, and as a result, the hardware reflects that of what is reasonable in a typical desktop machine. I think it would have been interesting, and possibly more reflective of the area I am trying to influence with this research, if the testing could have been done on a purpose built server machine, that may have had more than 16GB of RAM, and possibly a more server focused processor. For example; AMD have recently written a paper showing the use of their new EPYC line-up of server CPUs for hosting containers \citep{amdcontainers}.

However, if we are looking to make recommendations for users that may still use old server hardware, I believe the use of older desktop hardware in this test may actually be \emph{more} compelling to them, as the results may offer those users an alternative should they already be using virtualisation, and looking to get a little bit more usage out of the hardware before an upgrade.

Perhaps if the test was done over a few different systems, we could have compared the usefulness of containers and virtual machines across these systems. This however, would have made the project much larger, and would have been outside the scope of what could be considered a reasonable amount of work.

\subsubsection{The Client}
\label{ClientHardwareLimitation}
As already discussed, access to hardware switches, and with that, the ability to run the network infrastructure outside of a virtual network was not possible. This meant the client within the network had to be hosted on the same hardware as the rest of the network. Obviously, in a real-world environment we would expect a number of clients to be on the same network, but on separate host hardware.

It could be argued that by having the client sitting on the same machine, that the results are skewed, as delay to the processing as a result of the client using CPU time and RAM could affect how the other virtual machines or containers run. This affects objective nine within the Terms of Reference (appendix \ref{appendix:tor}), as it may result in inaccuracies within the data. Part of the way through the design process, it was decided that to ensure that there was no run-off affect on the results, that the client would be the same VMware virtual machine with the same number of resources allocated to it for both tests. By doing so, we ensured that any offset to the results was the same for \emph{both} systems, and thus partially mitigated the affects to objective nine.

There was still some possibility that having VMware running and Docker running at the same time could introduce some sort of fighting for resources between that could negatively affect the Docker results, but given the immense head run that Docker has on VMware, I feel any affect on the results here is negligible, and that we are still in line to have achieved objective nine.

\subsubsection{Using VMware's VMnet8 Adapter}%Mention it would have been better to use a real network with a switch etc.
Another point of contention within the research is the use of VMware's NAT network adapter `VMnet8'. The use of this adapter in a link-local state to simulate an environment for the Docker containers could both be seen as a way in which undermining of the results was avoided, but also the contrary.

This is because using the VMnet8 adapter allowed Docker to use the same network environment as the VMware tests. This in theory should remove any discrepancy in results that would result from the network medium instead of the distinction between VMware and Docker. However, it could also be argued that VMware virtual machines using the NAT network directly through VMware could have different performance to Docker containers using the adapter. It could be possible theoretically that the Docker containers are at a disadvantage by using the adapter, due to hidden variables such as bottlenecks in the pass through of traffic over the VMnet8 adapter onto the VMware NAT network.

When looking at the results however, it is clear that any disadvantage to Docker that may be present, won't have changed the main conclusive point of the data. That being that Docker is the better, faster and less resource heavy option for network management.

\section{Evaluation of the test results}
\label{sec:EvaluationTestResults}
One of the most important parts of the tests in my view is the quantity of data that was generated. Testing was done across the whole system, and those tests focused on both the network performance, and the performance of the hardware itself. This makes the research very valuable, as it gives us a very detailed insight into how containerisation affects all parts of a network topology.

\subsection{Understanding the test results}
All of the tests we performed looked at some degree, into the performance of various network components, with Test 4 offering further insight into the hardware performance of the system whilst performing network tasks. To understand the relationship between the results, we will start with Test 1 (iPerf3, Section \ref{sec:Test1}). Here, the tabled results showed (as with all other tests) a clear win for Docker, with an average transfer rate sitting at 116.06 MBytes/s higher than that of VMware's. Looking into the transfer rate over time however, we can see that performance over time for VMware and Docker was not a linear and parallel, like we may expect to see. Instead, whilst Docker's Apache server transfer rate \emph{was} relatively linear, staying around the same level throughout the test with little deviation, we found that VMware's transfer fluctuated. At times, the transfer rate for the VMware Apache server reached and even surpassed Docker's own transfer rate in the same relative time period, but it would then quickly drop back down to levels that were less than half of what it had been.

This sort of behaviour is something we then started to see in further tests, such as Test 2 (Sysbench MySQL I/O, Section \ref{sec:Test2}). In this test, Docker is again the clear winner when looking at averages, with VMware falling behind with around a third of the total queries performed in the same time frame. When we dig deeper into these results however, we see the same trend of fluctuation appearing again as we see that VMware has a minimum latency that is actually slightly smaller than Docker's minimum, but the average and 95th percentile latencies of VMware are far higher than the Docker Average's and 95th percentile latencies. This shows that the range of results for VMware is considerably larger, suggesting fluctuation like we saw in test one, whilst Docker's results maintain a small range, which suggests those results are more consistent.

Moving onto Test 3 (Namebench, Section \ref{sec:Test3}), we see that the Name Servers on the Docker systems can perform queries \(\approx72\%\) faster than the same Name Servers on the VMware system. Where this test falls down, is the inability to dig further into the results in the same way as we did for Test One and Test Two, due to the way that Namebench compiles and presents the results. The data crunching that takes places to find the averages is behind a screen on Namebench, and the raw data is never presented to the user, instead, only the Minimum, Average and Maximum results are given. This is most likely because the Alexa Top Sites data that the test uses is a propriety data set that may be protected under rules of usage. Nonetheless, this results in a smaller workable data pool when compared to the rest of the tests, and whilst the data collected here is still valuable, it would have been more valuable if we could have looked in detail to see if we could pinpoint if the tendency towards fluctuation in the VMware results, and stability in the Docker results, is present in this test also. Further disappointment in the results from this particular test comes from the maximum latency that was given, which was 3500ms for both VMware and Docker. This result is most likely due to errors on Authoritative servers for one or more of the domains that we tested against resulting in the request being dropped after this exact amount of time (3.5 seconds), and as such, should be treated as anomalous data. Without access to the raw data, we can't actually confirm this and thus it remains merely as hypothesis. This kind of error is to be expected given that the test was against \(\approx 38,000\) domains, but access to the raw data would have allowed us to investigate whether or not it was the same domains giving this high output, or even if this error was more frequent on one system over another. The reason Namebench was chosen despite not giving much flexibility and control over the results, is merely due to the fact that there are so few DNS server performance tests. Perhaps the development of a more in-depth DNS benchmarking suite would be beneficial so that interesting behaviours such as those witnessed in this test can be explored in greater detail.

The final test was Test 4 (JMeter and Netdata, Section \ref{sec:Test4}). Though we grouped this as one test, we essentially performed two tests in one, those being the networking performance test with JMeter, and then the host machine's hardware performance measured by Netdata during that test (alongside comparison idle data from Netdata also). To start, we will evaluate the network performance measured from the JMeter test. This test was much better than the one in Test Three (namebench) in terms of providing us with a real dataset. The full results from this test were output by JMeter, and this allowed a more in depth analysis of the results. To start though, and as with the other tests, the data was compiled to show the average latency where, unsurprisingly, Docker came out on top again, being on average  3.03ms faster than the equivalent Apache server on the VMware system. When taking a deeper look into these results, like in Test One and Two, we see that the spread of results for VMware is much higher. Whilst Docker and VMware both have most of their results in the 1ms category, we see that VMware has far more results spread across the categories for higher latencies. This is significant because it further shows this trend of fluctuating results that we have witnessed in Tests One and Two. We can also see that Docker has a number of latencies (248) in the less than 1ms category (0ms), whilst VMware only has 34, which further works to solidify that Docker is just faster than VMware in these high stress networking tasks.

Whilst Docker always remains the fastest option, these tests also demonstrate another key finding: that Docker, \emph{in this system at least}, is far more \textbf{Reliable} than VMware. In Test One, Two and Three, we see a clear trend towards Docker not only performing well, but being consistent in doing so. VMware can often times pull results that match or surpass the Docker equivalent, it just doesn't maintain these results for any amount of time that could be considered reasonable for a live system in a critical environment and as anyone with Computer Networking knowledge should understand, reliability is a key factor with any live system, not to mention a high input and output OLTP database like the one we are trying to simulate. As discussed in the Analysis and Synthesis, OLTP systems are very common ways of maintaining databases with high user interaction, as as such they need to be able to manage under a large load, as to not crash or develop errors during peak usage times.

A good way of seeing how often these errors take place for both systems is looking at results which appear to have excessively long latencies (as extremely high latencies would suggest the system is waiting for other resources, which further leads to I/O errors). When looking at Docker, we see than only one result appears to be due to an error, when on the VMware system there appears to be seven results that are due to errors. It is worth a note here mentioning that this method of determining errors is speculation; no actual MySQL error checking was used during the tests. Other reasons could be to blame, but it stands to reason that the most likely reason is I/O errors.

\subsection{Understanding the fluctuations}
By now it is clear that the results present Docker as faster on average, but we have not yet fully understood why the fluctuations on the VMware system that we are observing in tests one, two, and four take place.

This may be one of the areas that the research falls down, as no scientifically sound root cause analysis is possible with the results we have. Currently we can only hypothesise as to why the results present in this way. Despite this, we \emph{can} make hypothesis as to the most likely and logical explanation, that explanation being linked to the RAM (and possibly, but perhaps less so, the CPU) usage. As discussed in subsection \ref{HardwareLimitations}, testing was only performed on one Desktop PC with limited resources. If we then look to the second stage (Netdata results) within Test Four, we can the CPU and RAM usage, along with Load on the host machine during the JMeter test and when idle. Looking at RAM specifically, we can see that during the VMware testing there is little to no free RAM. It is most likely, by my own hypothesis, that this is the reasoning behind the seemingly random high latencies, and drops in throughput. Furthermore, the Load increase on the VMware system is considerable, and most likely due to the system needing to quickly to perform a lot of memory swapping (moving memory from RAM to Disk). When this process of swapping memory back and fourth becomes large, then it starts to have an affect on CPU times (reflected in load as measured by Netdata), at this point, we are experiencing what is known as thrashing \citep{thrashing}.

To summarise, whilst Docker is still more suitable than VMware (as it \emph{does} uses less resources), it could be that our results around network performance are being unfair to VMware in that we are not providing enough resources for it to be able to handle the workload we are putting it under. Whilst it could be hypothesised that we would still see similar results without this RAM bottleneck, this research doesn't investigate this any further.

That is not to say that the research isn't still useful, as this recorded behaviour could actually be very useful to some users (Businesses, Network Administrators, IT Infrastructure Workers, etc.). As there may be a number of those users using outdated hardware along with virtualisation, in order to host networks, that are beginning to show signs of the issues we detailed here (i.e. Memory Swapping or Thrashing). For those users, the research in this paper details a possible solution (that being containerisation) that they could do right now, without yet having to update to newer hardware. This option could an appealing solution for those looking to stay in-line with new demands in the networking sector without spending large amounts of money on new hardware.

Nevertheless, should this research be done again I would want to look to do testing on other, more advanced hardware, to confirm that the results remain similar. Similar research that removed hardware bottlenecks that are present in this research could be extremely useful, as it could be better applied to those that are already running modern and fast server machines with ample memory. Sadly, with where the results we see in this paper lay, we are less useful to those very end users.

%---------------------------


\chapter{Evaluation of the project and process}%What has the project done for me, what have i learned, what have i struggled with?
I believe that this project has been the most defining aspect of my university degree, and of my academic development to date. I have been confident in my writing ability throughout my academic journey, but this project has challenged my writing ability in such a way that I have been forced to self-critique and develop it further. This project has also made me develop numerous skills which I hope to take forward with me into my future work, weather that be in academia or elsewhere. The ways I have improved my abilities as a result of this year long project are numerous, but I hope to touch on them in the following sections.

It is also worth noting here that I hope to touch on my failings as well as my successes. This project has been a way for me to show my skills, and my ability to work on substantial projects; whilst I would love to be able to say that I am a perfect student that makes no mistakes, this would simply be untrue. A good evaluation of one's own ability requires that person to be open about their downfalls.

\section{Development of skills}
\subsection{VMware}%Mention that I already had VMware knowledge, and expected this part to be easy. Suggest that I have still learned a lot about vmware's workings here.
Working with Operating Systems using VMware was the starting point for my research idea. I had previously worked with VMware, and already felt confident in my ability to produce a VMware based network to a good standard. As a result, the work done around VMware is of very good quality, and I had no issue in achieving the objective to build the virtual machine topology (as laid out in my Terms of Reference objectives). However, I do feel that I have still developed my skills surrounding VMware a considerable amount. The main area of learning being a deeper understanding of the way that VMware's various network modes operate. Before doing this research, I knew how to use these network modes to achieve different results, but I had taken no thought into how they actually work on an Operating Systems or hardware level. I had also never used VMware's Network manager to change aspects of these virtual networks such as disabling automatic DHCP or changing the default IP addressing schemes (both of which were required during the project). Now that I have finished the practical work in the project, I can use hindsight to say that though I did already have a good understanding of how to \emph{use} VMware to create the VMware network, I didn't have the in-depth \emph{knowledge of why} VMware worked, like I do now. When comparing this train of thought to my Terms of Reference Project Plan, it is clear than I should have factored this in. The plan makes little mention of learning VMware skills. Instead, it only makes mention of Docker. This perhaps was due to a level of naivety towards the scale of the project. That is to say, that I underestimated the level of knowledge I would require around VMware in order to fulfil the VMware tasks I set out to accomplish within the project.

Understanding how VMware networks operate also helped me when wanting to test the Docker network system. I had the unique problem of not having a way to network the various bits of traffic between docker containers, as Docker's own network modes didn't easily allow traffic which relies on Unicast, Multicast and Broadcast (Such as DHCP) unless routed to a real network using \texttt{macvlan}. My developed knowledge of VMware gave me the idea to route the Docker traffic, using \texttt{macvlan}, to the VMnet8 network adapter, and setting the adapter as a link-only network interface. This in-turn gave me more reliable results, as it allowed me to rule out latency of the network medium as a variable, as both systems ended up using the same VMware NAT network.

Had I not developed my VMware network knowledge, I wouldn't have known that this was an option, and the research as a whole would have suffered.

\subsection{Docker}%Mention usage of docker in other fields and how useful that will be. Mention use of Docker Networks etc
When starting the project, my understanding of Docker was very minimal. I roughly understood why containers were different to virtual machines, and I had heard of Docker being used for development purposes, but outside of this, I had no understanding of how Docker actually worked.

Taking a look at what I am capable of now, I am extremely pleased with my development of Docker knowledge and skills. I am now very confident in the writing of Dockerfiles, Docker image management, Docker Networking and more. As a direct result of this project work, I believe I have given myself deep insight into an area that should serve me well in my future work. Docker has gained a lot of popularity in recent years, and I am sure that this will continue to be the case into the future, as more and more services come to use it. As a result of the experiences I have had with Docker through this project work, I have gained valuable insight into an area that I am sure will serve me well in years to come.

This sits in line with the objective set in my Terms of Reference (See appendix) whereby I aimed to learn how to `implement and utilise' Docker containers.

\subsection{Benchmarking Software}
There are two areas within my objectives that relate to my development of skills in benchmarking tools, these being to determine a method for evaluating performance of the two systems, and to accurately measure performance of the systems. I have already laid out both successes and failings of my benchmarking and testing in section \ref{sec:EvaluationTestResults}, but it is important to talk about the development of my skills in this area, irregardless of the actual test results.

Firstly, all of the toolsets and programs I have used to benchmark the system were new to me. I had no experience with iPerf3, Sysbench, Namebench, JMeter, or Netdata before taking on this project, and as such, I had to perform a number of test runs and go through a lot of documentation when using these tools in order to learn how they worked so that I could collect the data needed for the my comparisons. This reflects the first objective mentioned in this subsection; ``to determine a method for evaluating performance of the two systems''. It also goes further though, as it I required my own effort to adapt and learn these new tools. This is something I feel I may have misjudged when designing my project plan. I made sufficient time for learning Docker, and even to learn about different benchmarks as part of the decision making process for which benchmarks I would use for my work, but I failed to account for the time it would take me to learn these various benchmarks. If I was to do the project again, I would like to spend more time looking at different benchmarks in more detail, so see if I could tune them to get more results. For example, both JMeter and Sysbench are extensive benchmarking suites; the tests I have performed with them in this research are just scratching the surface of what they can do, and I would love to work with them more in the future to see if I can generate some more interesting data.


Despite this, I am of the opinion that the data produced in this project speaks to my ability to pickup new tools quickly and sufficiently, and that this further reflects the second objective I mentioned in this section; ``to accurately measure performance of the systems''. Most of the tests I performed gave good and accurate results that allowed for detailed comparisons of the two systems. I also think that using the method of using a selection of tests was a benefit to the process of data collection within the project, as it allowed a greater spread of data, with various tests picking up the shortcomings of others in order to create a more well-rounded and concrete benchmark.

\section{Personal Evaluation}%Talk about your personal achievements, struggles, etc. Ability to time keep! Covid pandemic?
The first part of the project report to be written was the analysis. During this stage, I struggled with the literature review, as it is something I am not particularly used to doing. I have done a lot of technical work, and as such written technical reports. Of course I use references during these reports, but I rarely go into intricate detail around these studies, reports and papers, and so doing that for this research was something I struggled with. This had a knock-on affect towards the rest of my work in the early stages. My synthesis was pushed back as I failed to manage my time well around my Analysis. Time management is something I have always struggled with in the past however, so I do have my coping methods for when these things happen, as such I have managed to pull back time to sink into this project in a way that hasn't been detriment to the quality of my work. My technical abilities in computing which allowed me to pick up Docker, and the benchmarking suites quickly, were also key to my success here. Should I ever do this project or similar projects in the future, I would be more aware that I need to allocate more time to analysis, as it is something I am still not entirely confident in. If i hadn't been able to take on the project synthesis work in the way that I did, I feel the project may have suffered.

Another area worth mentioning is note taking. I think it would have been useful for me throughout my research, to have kept a sort of diary of thoughts around topics I would like to discuss. Often times my thoughts can be sporadic and messy, and I think a research diary would have helped collate some of these thoughts when writing my dissertation. Instead, there were times when I forgot about certain points that I would have liked to have made, or left sentences unfinished after losing my train of thought. This is something I have struggled with in the past, so I feel I should have made better efforts to maintain some degree of direction between writing sections. That being said, whilst my sporadic and sometimes irregular way of working can be a hindrance to my output, it's important to state that it is also this way of thinking that leads me into research topics and areas that I find interesting. So that is to say, that whilst my way of working could have been managed better, I don't think that it is inherently a bad way of working.

Another area that I overlooked that also set me back was a struggle I had around deciding the best way to create and manage the systems. In my terms of reference, I had initially planned to use a remote access virtual machine infrastructure that would be provided by the university. Once I started to plan for the research further, I realised this was not the the right option. This was because the virtual machines were a key part of the testing, not just the medium for testing to take place. If I wanted to be able to measure performance of virtual machines and then compare that to Containers, it was clear that the hardware would need to remain the same between tests, and that using remote virtual machines, with no direct access to the host machine, was not going to work. This again links back to the points I have made surrounding issues with the hardware of the machine I was using. Whilst using my personal computer as the host machine wasn't a perfect solution, I do believe that this decision was the right one, as had I continued on the trajectory set in my Terms of Reference document, I feel my final results would not have been as reliable.

One advantage to my project plan was the fact I made time for troubleshooting during my synthesis as part of my learning time. This was useful as it meant I was under less pressure when things went wrong during the creation of machines, or during testing. This is something I would certainly take forward with me into future projects, as I understand that learning only takes place through failure, and that giving myself time to cope when things go wrong is important to the process of project of this scale, especially when using newly learned skills, or when self-teaching.


Despite the above mentioned flaws in my process I still believe that I have created an impressive piece of research, and I am extremely proud of the work I have created.