%!TeX root=../Dissertation.tex
%!TeX bibfile=./synthesis.bib

%------

\chapter{Evaluation of the Product}
%Do a little preamble about success and important results to get us started
\section{Evaluation of the systems}
\subsection{Successes}
Both systems (The VMware system and the Docker system) do exactly what was described in the analysis. The network topology for both of the real systems is exactly the same, with both of these networks matching the topology that was designed during the analysis section (Figure \ref{fig:Topology}, in subsection \ref{subsec:TopologyDiagram}). The system maintained a LAMP topology, as was specified, along with the various other components of the system such as DHCP and DNS. This is an important part of the research because it demonstrates that this kind of system can be supported by containers, and not just virtual machines, as is done historically. Furthermore, the configuration for matching services across the two systems (ie. DHCP on VMware; DHCP on Docker) are identical, ensuring that results remained comparable.

Separately, the VMware system is not particularly impressive, or groundbreaking, whilst the container based system on the other hand \emph{is} impressive, and potentially groundbreaking. This is because there are not many examples of systems such as this one being built for use outside of small scale development operations (whereby the network infrastructure couldn't be supported on a real LAN). Together, the two systems combine to form an important base for research that could potentially impact the current paradigm of virtualisation in which we currently find ourselves in. The actual results, and their impact will be discussed further in section \ref{sec:EvaluationTestResults}, but without the work that was conducted to ensure that both systems worked exactly as designed, those results wouldn't have been useful. This is due to the fact that the testing was a direct comparison of systems; should there have been any disparity between the VMware network and the Docker network, the results would not be a fair comparison.

This required a level of perseverance when it came to finding something within the Docker system that was difficult. For example, getting phpMyAdmin to work on the Docker system wasn't as easy as the rest of the build (as mentioned in subsection \ref{subsec:softwaresynth}), but a workaround was found to make it work, even though there was probably an alternative system that could have been implemented easier. Had the Docker system been the only system being built, and that had been the project basis, then using an alternative method would have been acceptable, and this could have been worked into the synthesis, but with both systems needing to be identical, this was just not an option. In the long run, I think this has made the project more of a success, because it has proven that taking systems that already exist within virtual environments, and converting them to container environments is an option without compromise for those that are in positions to make this move.

The scalability of the system we have created is very good. During development the system was actually entirely designed and created on a separate machine and then moved over to the testing machine after the fact with a fresh installation of Ubuntu. Setting the whole system up on a fresh machine was simple, and there was minimal hitches in the process. This suggests that both the containers and virtual machines were designed and created in an efficient manner. I would suggest that scaling up the number of machines, or adding new functions to the system later, would be an easy task.

\subsection{Some notable Limitations}%talk about ram being bottlenecked, using an old system, what would you have liked to have done?
\label{HardwareLimitations}
This project took place over what became a bizarre year for academia. Due to the Coronavirus pandemic, there was limited ability to access network labs and University infrastructure, so this project had to be scaled in a way that was reasonable for me to complete with my own personal hardware.

\subsubsection{RAM and CPU usage}
\label{RAMCPU}
The computer that was used was a desktop PC, and as a result, the hardware reflects that of what is reasonable in a typical desktop machine. I think it would have been interesting, and possibly more reflective of the area I am trying to influence with this research, if the testing could have been done on a purpose built server machine, that may have had more than 16GB of RAM, and possibly a more server focused processor. For example; AMD have recently written a paper showing the use of their new EPYC line-up of server CPUs for hosting containers \citep{amdcontainers}.

However, if we are looking to make recommendations for users that may still use old server hardware, I believe the use of older desktop hardware in this test may actually be \emph{more} compelling to them, as the results may offer those users an alternative should they already be using virtualisation, and looking to get a little bit more usage out of the hardware before an upgrade.

Perhaps if the test was done over a few different systems, we could have compared the usefulness of containers and virtual machines across these systems. This however, would have made the project much larger, and would have been outside the scope of what could be considered a reasonable amount of work.

\subsubsection{The Client}
\label{ClientHardwareLimitation}
As already discussed, access to hardware switches, and with that, the ability to run the network infrastructure outside of a virtual network was not possible. This meant the client within the network had to be hosted on the same hardware as the rest of the network. Obviously, in a real-world environment we would expect a number of clients to be on the same network, but on separate host hardware.%TODO need to go back over this section.
%Talk about virtual machine being used on the host machine.
Part of the way through the design process, it was decided that to ensure that there was no run-off affect on the results

\subsubsection{Using VMware's VMnet8 Adapter}%Mention it would have been better to use a real network with a switch etc.

\section{Evaluation of the test results}
\label{sec:EvaluationTestResults}
One of the most important parts of the tests in my view is the quantity of data that was generated. Testing was done across the whole system, and those tests focused on both the network performance, and the performance of the hardware itself. This makes the research very valuable, as it gives us a very detailed insight into how containerisation affects all parts of a network topology.

\subsection{Understanding the test results}
All of the tests we performed looked at some degree, into the performance of various network components, with Test 4 offering further insight into the hardware performance of the system whilst performing network tasks. To understand the relationship between the results, we will start with Test 1 (iPerf3, Section \ref{sec:Test1}). Here, the tabled results showed (as with all other tests) a clear win for Docker, with an average transfer rate sitting at 116.06 MBytes/s higher than that of VMware's. Looking into the transfer rate over time however, we can see that performance over time for VMware and Docker was not a linear and parallel, like we may expect to see. Instead, whilst Docker's Apache server transfer rate \emph{was} relatively linear, staying around the same level throughout the test with little deviation, we found that VMware's transfer fluctuated. At times, the transfer rate for the VMware Apache server reached and even surpassed Docker's own transfer rate in the same relative time period, but it would then quickly drop back down to levels that were less than half of what it had been.

This sort of behaviour is something we then started to see in further tests, such as Test 2 (Sysbench MySQL I/O, Section \ref{sec:Test2}). In this test, Docker is again the clear winner when looking at averages, with VMware falling behind with around a third of the total queries performed in the same time frame. When we dig deeper into these results however, we see the same trend of fluctuation appearing again as we see that VMware has a minimum latency that is actually slightly smaller than Docker's minimum, but the average and 95th percentile latencies of VMware are far higher than the Docker Average's and 95th percentile latencies. This shows that the range of results for VMware is considerably larger, suggesting fluctuation like we saw in test one, whilst Docker's results maintain a small range, which suggests those results are more consistent.

Moving onto Test 3 (Namebench, Section \ref{sec:Test3}), we see that the Name Servers on the Docker systems can perform queries \(\approx72\%\) faster than the same Name Servers on the VMware system. Where this test falls down, is the inability to dig further into the results in the same way as we did for Test One and Test Two, due to the way that Namebench compiles and presents the results. The data crunching that takes places to find the averages is behind a screen on Namebench, and the raw data is never presented to the user, instead, only the Minimum, Average and Maximum results are given. This is most likely because the Alexa Top Sites data that the test uses is a propriety data set that may be protected under rules of usage. Nonetheless, this results in a smaller workable data pool when compared to the rest of the tests, and whilst the data collected here is still valuable, it would have been more valuable if we could have looked in detail to see if we could pinpoint if the tendency towards fluctuation in the VMware results, and stability in the Docker results, is present in this test also. Further disappointment in the results from this particular test comes from the maximum latency that was given, which was 3500ms for both VMware and Docker. This result is most likely due to errors on Authoritative servers for one or more of the domains that we tested against resulting in the request being dropped after this exact amount of time (3.5 seconds), and as such, should be treated as anomalous data. Without access to the raw data, we can't actually confirm this and thus it remains merely as hypothesis. This kind of error is to be expected given that the test was against \(\approx 38,000\) domains, but access to the raw data would have allowed us to investigate whether or not it was the same domains giving this high output, or even if this error was more frequent on one system over another. The reason Namebench was chosen despite not giving much flexibility and control over the results, is merely due to the fact that there are so few DNS server performance tests. Perhaps the development of a more in-depth DNS benchmarking suite would be beneficial so that interesting behaviours such as those witnessed in this test can be explored in greater detail.

The final test was Test 4 (JMeter and Netdata, Section \ref{sec:Test4}). Though we grouped this as one test, we essentially performed two tests in one, those being the networking performance test with JMeter, and then the host machine's hardware performance measured by Netdata during that test (alongside comparison idle data from Netdata also). To start, we will evaluate the network performance measured from the JMeter test. This test was much better than the one in Test Three (namebench) in terms of providing us with a real dataset. The full results from this test were output by JMeter, and this allowed a more in depth analysis of the results. To start though, and as with the other tests, the data was compiled to show the average latency where, unsurprisingly, Docker came out on top again, being on average  3.03ms faster than the equivalent Apache server on the VMware system. When taking a deeper look into these results, like in Test One and Two, we see that the spread of results for VMware is much higher. Whilst Docker and VMware both have most of their results in the 1ms category, we see that VMware has far more results spread across the categories for higher latencies. This is significant because it further shows this trend of fluctuating results that we have witnessed in Tests One and Two. We can also see that Docker has a number of latencies (248) in the less than 1ms category (0ms), whilst VMware only has 34, which further works to solidify that Docker is just faster than VMware in these high stress networking tasks.

Whilst Docker always remains the fastest option, these tests also demonstrate another key finding: that Docker, \emph{in this system at least}, is far more \textbf{Reliable} than VMware. In Test One, Two and Three, we see a clear trend towards Docker not only performing well, but being consistent in doing so. VMware can often times pull results that match or surpass the Docker equivalent, it just doesn't maintain these results for any amount of time that could be considered reasonable for a live system in a critical environment and as anyone with Computer Networking knowledge should understand, reliability is a key factor with any live system, not to mention a high input and output OLTP database like the one we are trying to simulate. As discussed in the Analysis and Synthesis, OLTP systems are very common ways of maintaining databases with high user interaction, as as such they need to be able to manage under a large load, as to not crash or develop errors during peak usage times.

A good way of seeing how often these errors take place for both systems is looking at results which appear to have excessively long latencies (as extremely high latencies would suggest the system is waiting for other resources, which further leads to I/O errors). When looking at Docker, we see than only one result appears to be due to an error, when on the VMware system there appears to be seven results that are due to errors. It is worth a note here mentioning that this method of determining errors is speculation; no actual MySQL error checking was used during the tests. Other reasons could be to blame, but it stands to reason that the most likely reason is I/O errors.

\subsection{Understanding the fluctuations}
By now it is clear that the results present Docker as faster on average, but we have not yet fully understood why the fluctuations on the VMware system that we are observing in tests one, two, and four take place.

This may be one of the areas that the research falls down, as no scientifically sound root cause analysis is possible with the results we have. Currently we can only hypothesise as to why the results present in this way. Despite this, we \emph{can} make hypothesis as to the most likely and logical explanation, that explanation being linked to the RAM (and possibly, but perhaps less so, the CPU) usage. As discussed in subsection \ref{HardwareLimitations}, testing was only performed on one Desktop PC with limited resources. If we then look to the second stage (Netdata results) within Test Four, we can the CPU and RAM usage, along with Load on the host machine during the JMeter test and when idle. Looking at RAM specifically, we can see that during the VMware testing there is little to no free RAM. It is most likely, by my own hypothesis, that this is the reasoning behind the seemingly random high latencies, and drops in throughput. Furthermore, the Load increase on the VMware system is considerable, and most likely due to the system needing to quickly to perform a lot of memory swapping (moving memory from RAM to Disk). When this process of swapping memory back and fourth becomes large, then it starts to have an affect on CPU times (reflected in load as measured by Netdata), at this point, we are experiencing what is known as thrashing \citep{thrashing}.

To summarise, whilst Docker is still more suitable than VMware (as it \emph{does} uses less resources), it could be that our results around network performance are being unfair to VMware in that we are not providing enough resources for it to be able to handle the workload we are putting it under. Whilst it could be hypothesised that we would still see similar results without this RAM bottleneck, this research doesn't investigate this any further.

That is not to say that the research isn't still useful, as this recorded behaviour could actually be very useful to some users (Businesses, Network Administrators, IT Infrastructure Workers, etc.). As there may be a number of those users using outdated hardware along with virtualisation, in order to host networks, that are beginning to show signs of the issues we detailed here (i.e. Memory Swapping or Thrashing). For those users, the research in this paper details a possible solution (that being containerisation) that they could do right now, without yet having to update to newer hardware. This option could an appealing solution for those looking to stay in-line with new demands in the networking sector without spending large amounts of money on new hardware.

Nevertheless, should this research be done again I would want to look to do testing on other, more advanced hardware, to confirm that the results remain similar. Similar research that removed hardware bottlenecks that are present in this research could be extremely useful, as it could be better applied to those that are already running modern and fast server machines with ample memory. Sadly, with where the results we see in this paper lay, we are less useful to those very end users.

%---------------------------


\chapter{Evaluation of the project and process}%What has the project done for me, what have i learned, what have i struggled with?

\section{Development of skills}
\subsection{VMware}%Mention that I already had VMware knowledge, and expected this part to be easy. Suggest that I have still learned a lot about vmware's workings here.

\subsection{Docker}%Mention usage of docker in other fields and how useful that will be. Mention use of Docker Networks etc

\section{Personal Evaluation}%Talk about your personal achievements, struggles, etc. Ability to time keep! Covid pandemic?

%Mention I should have made more notes as I was going rather than trying to do it straight through in order. I followed the gantt, but didn't leave enough information between bits that I was doing.