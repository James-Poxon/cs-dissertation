%!TeX root=../Dissertation.tex
%!TeX bibfile=./synthesis.bib

%------

\chapter{Evaluation of the produced systems (Product)}
\section{Successes}%evaluate the system as it stands created. Talk about the quality of the system itself, not the stuff that could affect the testing necessarily, as we are mentioning that further down.
Both systems (The VMware system and the Docker system) do exactly what was described in the analysis. The network topology for both of the real systems is exactly the same, with both of these networks matching the topology that was designed during the analysis section (Figure \ref{fig:Topology}, in subsection \ref{subsec:TopologyDiagram}). The system maintained a LAMP topology, as was specified, along with the various other components of the system such as DHCP and DNS. This is an important part of the research because it demonstrates that this kind of system can be supported by containers, and not just virtual machines, as is done historically. Furthermore, the configuration for matching services across the two systems (ie. DHCP on VMware; DHCP on Docker) are identical, ensuring that results remained comparable.

Separately, the VMware system is not particularly impressive, or groundbreaking, whilst the container based system on the other hand \emph{is} impressive, and potentially groundbreaking. This is because there are not many examples of systems such as this one being built for use outside of small scale development operations (whereby the network infrastructure couldn't be supported on a real LAN). Together, the two systems combine to form an important base for research that could potentially impact the current paradigm of virtualisation in which we currently find ourselves in. The actual results, and their impact will be discussed further in Chapter \ref{chap:EvaluationTestResults}, but without the work that was conducted to ensure that both systems worked exactly as designed, those results wouldn't have been useful. This is due to the fact that the testing was a direct comparison of systems; should there have been any disparity between the VMware network and the Docker network, the results would not be a fair comparison.

This required a level of perseverance when it came to finding something within the Docker system that was difficult. For example, getting phpMyAdmin to work on the Docker system wasn't as easy as the rest of the build (as mentioned in section \ref{subsec:softwaresynth}), but a workaround was found to make it work, even though there was probably an alternative system that could have been implemented easier. Had the Docker system been the only system being built, and that had been the project basis, then using an alternative method would have been acceptable, and this could have been worked into the synthesis, but with both systems needing to be identical, this was just not an option. In the long run, I think this has made the project more of a success, because it has proven that taking systems that already exist within virtual environments, and converting them to container environments is an option without compromise for those that are in positions to make this move.

\section{Limitations}%talk about ram being bottlenecked, using an old system, what would you have liked to have done?
\label{HardwareLimitations}
This project took place over what became a bizarre year for academia. Due to the Coronavirus pandemic, there was limited ability to access network labs and University infrastructure, so this project had to be scaled in a way that was reasonable for me to complete with my own personal hardware.

\subsection{RAM and CPU usage}
\label{RAMCPU}
The computer that was used was a desktop PC, and as a result, the hardware reflects that of what is reasonable in a typical desktop machine. I think it would have been interesting, and possibly more reflective of the area I am trying to influence with this research, if the testing could have been done on a purpose built server machine, that may have had more than 16GB of RAM, and possibly a more server focused processor. For example; AMD have recently written a paper showing the use of their new EPYC line-up of server CPUs for hosting containers \citep{amdcontainers}.

However, if we are looking to make recommendations for users that may still use old server hardware, I believe the use of older desktop hardware in this test may actually be \emph{more} compelling to them, as the results may offer those users an alternative should they already be using virtualisation, and looking to get a little bit more usage out of the hardware before an upgrade.

Perhaps if the test was done over a few different systems, we could have compared the usefulness of containers and virtual machines across these systems. This however, would have made the project much larger, and would have been outside the scope of what could be considered a reasonable amount of work.

\subsection{The Client}
\label{ClientHardwareLimitation}
As already discussed, access to hardware switches, and with that, the ability to run the network infrastructure outside of a virtual network was not possible. This meant the client within the network had to be hosted on the same hardware as the rest of the network. Obviously, in a real-world environment we would expect a number of clients to be on the same network, but on separate host hardware. Mitigations to this fact were made in an attempt to counteract any affect this would have on the results, which is explored in more detail in subsection \ref{compr:client}, when we look at the results.

\section{Flexibility of the system}

Despite the aforementioned concerns with the product, the scalability of the system we have created is very good. During development the system was actually entirely designed and created on a separate machine and then moved over to the testing machine after the fact with a fresh installation of Ubuntu. Setting the whole system up on a fresh machine was simple, and there was minimal hitches in the process. This suggests that both the containers and virtual machines were designed and created in an efficient manner. I would suggest that scaling up the number of machines, or adding new functions to the system later, would be an easy task.




%-------------------------



\chapter{Evaluation of the test results}
\label{chap:EvaluationTestResults}
%What is interesting? What has the most impact? Mention that there is lots of data that tests all parts of the system, not just one part.
One of the most important parts of the tests in my view is the quantity of data that was generated. Testing was done across the whole system, and those tests focused on both the network performance, and the performance of the hardware itself. This makes the research very valuable, as it gives us a very detailed insight into how containerisation affects all parts of a network topology.

\section{Understanding the test results}
All of the tests we performed looked at some degree, into the performance of various network components, with Test 4 offering further insight into the hardware performance of the system whilst performing network tasks. To understand the relationship between the results, we will start with Test 1 (iPerf3, Section \ref{sec:Test1}). Here, the tabled results showed (as with all other tests) a clear win for Docker, with an average transfer rate sitting at 116.06 MBytes/s higher than that of VMware's. Looking into the transfer rate over time however, we can see that performance over time for VMware and Docker was not a linear and parallel, like we may expect to see. Instead, whilst Docker's Apache server transfer rate \emph{was} relatively linear, staying around the same level throughout the test with little deviation, we found that VMware's transfer fluctuated. At times, the transfer rate for the VMware Apache server reached and even surpassed Docker's own transfer rate in the same relative time period, but it would then quickly drop back down to levels that were less than half of what it had been.

This sort of behaviour is something we then started to see in further tests, such as Test 2 (Sysbench MySQL I/O, Section \ref{sec:Test2}). In this test, Docker is again the clear winner when looking at averages, with VMware falling behind with around a third of the total queries performed in the same time frame. When we dig deeper into these results however, we see the same trend of fluctuation appearing again as we see that VMware has a minimum latency that is actually slightly smaller than Docker's minimum, but the average and 95th percentile latencies of VMware are far higher than the Docker Average's and 95th percentile latencies. This shows that the range of results for VMware is considerably larger, suggesting fluctuation like we saw in test one, whilst Docker's results maintain a small range, which suggests those results are more consistent.

Moving onto Test 3 (Namebench, Section \ref{sec:Test3}), we see that the Name Servers on the Docker systems can perform queries \(\approx72\%\) faster than the same Name Servers on the VMware system. Where this test falls down, is the inability to dig further into the results in the same way as we did for Test One and Test Two, due to the way that Namebench compiles and presents the results. The data crunching that takes places to find the averages is behind a screen on Namebench, and the raw data is never presented to the user, instead, only the Minimum, Average and Maximum results are given. This is most likely because the Alexa Top Sites data that the test uses is a propriety data set that may be protected under rules of usage. Nonetheless, this results in a smaller workable data pool when compared to the rest of the tests, and whilst the data collected here is still valuable, it would have been more valuable if we could have looked in detail to see if we could pinpoint if the tendency towards fluctuation in the VMware results, and stability in the Docker results, is present in this test also. Further disappointment in the results from this particular test comes from the maximum latency that was given, which was 3500ms for both VMware and Docker. This result is most likely due to errors on Authoritative servers for one or more of the domains that we tested against resulting in the request being dropped after this exact amount of time (3.5 seconds), and as such, should be treated as anomalous data. Without access to the raw data, we can't actually confirm this and thus it remains merely as hypothesis. This kind of error is to be expected given that the test was against \(\approx 38,000\) domains, but access to the raw data would have allowed us to investigate whether or not it was the same domains giving this high output, or even if this error was more frequent on one system over another. The reason Namebench was chosen despite not giving much flexibility and control over the results, is merely due to the fact that there are so few DNS server performance tests. Perhaps the development of a more in-depth DNS benchmarking suite would be beneficial so that interesting behaviours such as those witnessed in this test can be explored in greater detail.

The final test was Test 4 (JMeter and Netdata, Section \ref{sec:test4}). This is the test where we essentially receive two different types of results, those being the networking performance, and then the host machine's hardware performance. To start, we evaluate the network performance measured from the JMeter test. This test was much better than the one in Test Three (namebench) in terms of providing us with a real dataset. The full results from this test were output by JMeter, and this allowed a more in depth analysis of the results. To start though, and as with the other tests, the data was compiled to show average

This fluctuating behaviour may be explained by


\section{Possible compromises}

\subsection{Using a virtual machine as the client}%Talk about virtual machine being used on the host machine.
\label{compr:client}
As a mentioned in subsection \ref{ClientHardwareLimitation} the client had to be 

Part of the way through the design process, it was decided that to ensure that there was no run-off affect on the results


\subsection{Using VMware's VMnet8 Adapter}%Mention it would have been better to use a real network with a switch etc.



%---------------------------


\chapter{Evaluation of the project and process}%What has the project done for me, what have i learned, what have i struggled with?

\section{Developed understanding of VMware's Networking}

\section{Learning Docker}

\section{Personal Evaluation}%Talk about your personal achievements, struggles, etc. Ability to time keep! Covid pandemic?